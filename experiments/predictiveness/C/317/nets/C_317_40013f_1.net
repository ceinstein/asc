FANN_FLO_2.1
num_layers=3
learning_rate=0.699804
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=11
cascade_activation_steepnesses=5.00000000000000000000e+00 3.21000000000000000000e+02 3.22000000000000000000e+02 3.23000000000000000000e+02 3.24000000000000000000e+02 1.15400000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 
layer_sizes=6 6 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.43372803926467895508e-01) (1, 1.82419270277023315430e-01) (2, -2.82282121479511260986e-02) (3, 6.19017258286476135254e-02) (4, -1.06000587344169616699e-01) (5, -4.32919152081012725830e-02) (0, -7.66258761286735534668e-02) (1, 1.78065188229084014893e-02) (2, 7.80509859323501586914e-02) (3, -1.21078435331583023071e-02) (4, -1.23159795999526977539e-01) (5, -9.33489017188549041748e-03) (0, 6.12440705299377441406e-02) (1, 6.12206757068634033203e-04) (2, -6.79515749216079711914e-02) (3, 9.78282745927572250366e-03) (4, -1.00917510688304901123e-01) (5, -9.99469161033630371094e-02) (0, 1.65035024285316467285e-01) (1, 2.95011233538389205933e-03) (2, -5.02773560583591461182e-03) (3, -8.42338800430297851562e-02) (4, 3.26025895774364471436e-02) (5, 3.64580526947975158691e-02) (0, -6.09814785420894622803e-02) (1, -3.84966544806957244873e-02) (2, 5.65825365483760833740e-02) (3, -5.96817731857299804688e-02) (4, 4.80380952358245849609e-02) (5, -7.50911384820938110352e-02) (6, 1.30399898625910282135e-03) (7, -2.11437437683343887329e-02) (8, -2.00133979320526123047e-01) (9, 6.97309225797653198242e-02) (10, -1.51632875204086303711e-02) (11, 1.49374091625213623047e+00) (6, 3.93232479691505432129e-02) (7, -6.94312900304794311523e-02) (8, 6.66821971535682678223e-02) (9, 1.47531107068061828613e-01) (10, -7.44584202766418457031e-02) (11, -1.13095343112945556641e-02) (6, 1.24461419880390167236e-01) (7, 6.02727457880973815918e-02) (8, 9.37576666474342346191e-02) (9, 4.14962470531463623047e-02) (10, -7.76756256818771362305e-02) (11, -7.95614421367645263672e-01) (6, 1.27990752458572387695e-01) (7, 7.69374668598175048828e-02) (8, -2.12087724357843399048e-02) (9, -5.27876056730747222900e-02) (10, 7.39115476608276367188e-02) (11, -1.24409806728363037109e+00) (6, 1.41074702143669128418e-01) (7, -3.98964844644069671631e-02) (8, -6.76116123795509338379e-02) (9, 4.35613058507442474365e-02) (10, -5.72151783853769302368e-03) (11, 2.91946232318878173828e-01) 
