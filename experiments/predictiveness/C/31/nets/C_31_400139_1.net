FANN_FLO_2.1
num_layers=3
learning_rate=0.698985
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=11
cascade_activation_steepnesses=5.00000000000000000000e+00 3.21000000000000000000e+02 3.22000000000000000000e+02 6.44000000000000000000e+02 7.68000000000000000000e+02 1.15400000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 
layer_sizes=6 6 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (6, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -2.72016882896423339844e+00) (1, -6.31149038672447204590e-02) (2, 4.55176122486591339111e-02) (3, 5.41350021958351135254e-02) (4, 8.76509323716163635254e-02) (5, 9.71918284893035888672e-01) (0, 1.90143775939941406250e+00) (1, 3.70060913264751434326e-02) (2, -1.81791782379150390625e-02) (3, -9.04133468866348266602e-02) (4, -8.54255855083465576172e-02) (5, -5.35607218742370605469e-01) (0, 1.98407304286956787109e+00) (1, -1.81080754846334457397e-02) (2, 1.41424657776951789856e-02) (3, -5.59081397950649261475e-02) (4, -3.81293855607509613037e-02) (5, -5.77025771141052246094e-01) (0, -2.78170371055603027344e+00) (1, 2.40753293037414550781e-02) (2, 2.47594248503446578979e-02) (3, -9.57068130373954772949e-02) (4, 4.67856898903846740723e-02) (5, 1.00072765350341796875e+00) (0, -1.07296633720397949219e+00) (1, 7.85201787948608398438e-02) (2, 7.44523992761969566345e-03) (3, -4.19918484985828399658e-02) (4, -9.75646302103996276855e-02) (5, 1.74248114228248596191e-01) (6, -5.73040008544921875000e-01) (7, 3.60662519931793212891e-01) (8, 3.74513119459152221680e-01) (9, -6.73165142536163330078e-01) (10, -2.72792279720306396484e-01) (11, 1.00327217578887939453e+00) (6, -1.41596710681915283203e+00) (7, 9.63380813598632812500e-01) (8, 1.12992489337921142578e+00) (9, -1.62388217449188232422e+00) (10, -4.91122186183929443359e-01) (11, -8.30302238464355468750e-01) (6, -1.53658485412597656250e+00) (7, 9.81145083904266357422e-01) (8, 1.07070410251617431641e+00) (9, -1.50743913650512695312e+00) (10, -4.97726470232009887695e-01) (11, -8.23554635047912597656e-01) (6, -1.54947078227996826172e+00) (7, 1.01113808155059814453e+00) (8, 1.03675055503845214844e+00) (9, -1.51507341861724853516e+00) (10, -4.84062820672988891602e-01) (11, -8.24569284915924072266e-01) (6, -1.54941141605377197266e+00) (7, 1.02936875820159912109e+00) (8, 9.47913289070129394531e-01) (9, -1.53238940238952636719e+00) (10, -5.91797053813934326172e-01) (11, -8.31930637359619140625e-01) 
