FANN_FLO_2.1
num_layers=3
learning_rate=0.699993
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=16
cascade_activation_steepnesses=0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 2.00000000000000000000e+00 6.08100000000000000000e+03 6.08200000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 6.00000000000000000000e+00 1.07597200000000000000e+06 1.07622800000000000000e+06 1.07654500000000000000e+06 1.07654600000000000000e+06 1.07680100000000000000e+06 1.07680200000000000000e+06 0.00000000000000000000e+00 
layer_sizes=9 9 9 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.47165916860103607178e-02) (1, 9.68835353851318359375e-02) (2, -4.22952957451343536377e-02) (3, 2.64792349189519882202e-02) (4, 6.61593824625015258789e-02) (5, -2.61622499674558639526e-02) (6, -9.88864377140998840332e-02) (7, 2.03715953975915908813e-02) (8, -1.09539143741130828857e-01) (0, -5.50532042980194091797e-02) (1, 1.42064569517970085144e-02) (2, -5.00774793326854705811e-02) (3, -4.39906120300292968750e-02) (4, 3.38811092078685760498e-02) (5, 2.92138215154409408569e-02) (6, -6.08249241486191749573e-03) (7, -7.85516202449798583984e-02) (8, -2.51410957425832748413e-02) (0, 1.79229825735092163086e-02) (1, -4.31993417441844940186e-02) (2, -6.89619034528732299805e-02) (3, 8.49057510495185852051e-02) (4, -7.13396817445755004883e-02) (5, -9.35043022036552429199e-02) (6, -1.12885579466819763184e-01) (7, -8.54449793696403503418e-02) (8, 3.14659439027309417725e-02) (0, -6.46769031882286071777e-02) (1, -4.10190112888813018799e-02) (2, -5.92074207961559295654e-02) (3, -5.52378520369529724121e-02) (4, 6.68059289455413818359e-02) (5, -6.23238794505596160889e-02) (6, 2.46685463935136795044e-03) (7, 3.54463141411542892456e-03) (8, -7.36229643225669860840e-02) (0, 7.21106827259063720703e-02) (1, 2.63561005704104900360e-03) (2, 4.47260513901710510254e-02) (3, 7.28310048580169677734e-02) (4, 5.44565357267856597900e-02) (5, -4.10674922168254852295e-02) (6, -7.72464722394943237305e-02) (7, -9.07480791211128234863e-02) (8, 9.96877476572990417480e-02) (0, 6.73875585198402404785e-02) (1, -6.13269209861755371094e-03) (2, -8.81659984588623046875e-02) (3, -5.89675381779670715332e-02) (4, -5.11149317026138305664e-02) (5, -3.13653424382209777832e-02) (6, -2.79294382780790328979e-02) (7, -5.26227280497550964355e-02) (8, 3.43897715210914611816e-02) (0, -6.22041597962379455566e-02) (1, -7.54301846027374267578e-02) (2, 3.90229113399982452393e-02) (3, -1.17151752114295959473e-01) (4, -6.96512237191200256348e-02) (5, 9.80039015412330627441e-02) (6, -7.63591751456260681152e-02) (7, -3.71711328625679016113e-02) (8, 3.52656915783882141113e-02) (0, 1.32046118378639221191e-02) (1, 7.02757462859153747559e-02) (2, -5.62096536159515380859e-02) (3, 2.72995978593826293945e-02) (4, 6.89238756895065307617e-02) (5, 4.64259497821331024170e-02) (6, -2.79743559658527374268e-02) (7, 3.36667560040950775146e-02) (8, 2.74199508130550384521e-02) (9, -8.60511604696512222290e-03) (10, -4.76890336722135543823e-03) (11, -1.15161687135696411133e-02) (12, -3.17718051373958587646e-02) (13, -3.40247862040996551514e-02) (14, 6.45531192421913146973e-02) (15, -2.28830315172672271729e-02) (16, -3.15179713070392608643e-02) (17, 1.16543024778366088867e-01) (9, 6.37987256050109863281e-02) (10, 6.45220279693603515625e-03) (11, -6.80311694741249084473e-02) (12, -3.90826910734176635742e-02) (13, 8.65493863821029663086e-02) (14, -6.00498914718627929688e-02) (15, -1.02804869413375854492e-01) (16, 7.26629272103309631348e-02) (17, 1.98774293065071105957e-01) (9, -8.83763059973716735840e-02) (10, -9.77881997823715209961e-02) (11, 8.41481387615203857422e-02) (12, 5.01396134495735168457e-02) (13, 6.28311792388558387756e-03) (14, 3.75352129340171813965e-02) (15, -1.08864799141883850098e-01) (16, 8.47070962190628051758e-02) (17, 1.86868339776992797852e-01) (9, 6.39149621129035949707e-02) (10, -5.34120798110961914062e-02) (11, 9.83481481671333312988e-02) (12, -2.87513360381126403809e-02) (13, 1.64685118943452835083e-02) (14, 2.95791886746883392334e-02) (15, 6.81067928671836853027e-02) (16, -8.08128118515014648438e-02) (17, -1.35965079069137573242e-01) (9, 2.90126912295818328857e-02) (10, -4.85368724912405014038e-03) (11, -6.57601505517959594727e-02) (12, 2.84032039344310760498e-02) (13, -1.13247428089380264282e-02) (14, 4.12646010518074035645e-02) (15, 6.27195239067077636719e-02) (16, 5.39110936224460601807e-02) (17, 2.31957495212554931641e-01) (9, -5.93467801809310913086e-02) (10, 6.01102821528911590576e-02) (11, 1.51789728552103042603e-02) (12, 3.52746546268463134766e-02) (13, 1.03036977350711822510e-01) (14, 1.72880906611680984497e-02) (15, 2.16992944478988647461e-02) (16, 5.69947734475135803223e-02) (17, 1.40137597918510437012e-01) (9, 8.59856046736240386963e-03) (10, 9.42989811301231384277e-02) (11, -6.49690181016921997070e-02) (12, 1.16989642381668090820e-01) (13, 2.70185302942991256714e-02) (14, -1.91265605390071868896e-02) (15, 1.12834602594375610352e-01) (16, 9.35863777995109558105e-02) (17, -1.16529166698455810547e-01) (9, 2.20810831524431705475e-03) (10, 5.12681938707828521729e-02) (11, 1.00895434617996215820e-01) (12, -1.88571996986865997314e-02) (13, -4.18164432048797607422e-02) (14, 3.10220271348953247070e-02) (15, 2.34319437295198440552e-02) (16, -8.00491347908973693848e-02) (17, -1.13505192101001739502e-01) 
