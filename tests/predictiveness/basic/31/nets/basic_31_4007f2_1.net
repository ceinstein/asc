FANN_FLO_2.1
num_layers=3
learning_rate=0.699993
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=16
cascade_activation_steepnesses=0.00000000000000000000e+00 0.00000000000000000000e+00 0.00000000000000000000e+00 2.00000000000000000000e+00 6.08100000000000000000e+03 6.08200000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 6.00000000000000000000e+00 1.07597200000000000000e+06 1.07622800000000000000e+06 1.07654500000000000000e+06 1.07654600000000000000e+06 1.07680100000000000000e+06 1.07680200000000000000e+06 0.00000000000000000000e+00 
layer_sizes=9 9 9 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (9, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (9, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.20604448020458221436e-02) (1, 5.68116903305053710938e-02) (2, -2.97150667756795883179e-02) (3, -4.28619459271430969238e-02) (4, -6.32465109229087829590e-02) (5, 5.14912605285644531250e-02) (6, -1.40048220753669738770e-01) (7, -6.48554787039756774902e-02) (8, -7.86275565624237060547e-02) (0, 3.92057299613952636719e-02) (1, -9.72559377551078796387e-02) (2, -2.09629964083433151245e-02) (3, -3.74587997794151306152e-02) (4, 2.01110523194074630737e-02) (5, 3.89682762324810028076e-02) (6, -2.10819933563470840454e-02) (7, 3.48626151680946350098e-02) (8, -9.76223051548004150391e-02) (0, 5.93405999243259429932e-02) (1, 8.50428715348243713379e-02) (2, -3.56600955128669738770e-02) (3, -6.90316259860992431641e-02) (4, 1.91080346703529357910e-02) (5, -1.07036000117659568787e-02) (6, -8.61957296729087829590e-02) (7, 2.12756637483835220337e-02) (8, 8.74970108270645141602e-03) (0, -1.78148113191127777100e-02) (1, 9.46485772728919982910e-02) (2, 6.58172741532325744629e-02) (3, 7.68405422568321228027e-02) (4, 4.97775636613368988037e-02) (5, 2.26289629936218261719e-02) (6, -5.28745278716087341309e-02) (7, -5.21400943398475646973e-02) (8, -9.75489988923072814941e-02) (0, 9.00066271424293518066e-02) (1, -5.33682666718959808350e-02) (2, -2.35844310373067855835e-02) (3, -4.76766303181648254395e-02) (4, -1.04928135871887207031e-01) (5, -2.08403710275888442993e-02) (6, 3.13603803515434265137e-02) (7, -2.66666822135448455811e-02) (8, -9.14949253201484680176e-02) (0, -5.37002906203269958496e-02) (1, 7.44574442505836486816e-02) (2, 6.55738115310668945312e-02) (3, -3.56023348867893218994e-02) (4, 9.72985476255416870117e-03) (5, 5.06166815757751464844e-02) (6, 2.87375710904598236084e-02) (7, 4.71448339521884918213e-02) (8, -5.43434768915176391602e-02) (0, -5.89349977672100067139e-02) (1, 9.79105234146118164062e-02) (2, -9.61063951253890991211e-02) (3, 5.62613196671009063721e-02) (4, -2.95123066753149032593e-02) (5, 9.85421761870384216309e-02) (6, 2.20785904675722122192e-02) (7, -5.47960139811038970947e-02) (8, 3.87117080390453338623e-02) (0, -4.02975901961326599121e-02) (1, -2.31100921519100666046e-03) (2, 9.19311568140983581543e-02) (3, -3.99708449840545654297e-02) (4, 1.69254792854189872742e-03) (5, -6.14371187984943389893e-02) (6, 3.64447273313999176025e-02) (7, 6.05017915368080139160e-02) (8, -5.23683205246925354004e-02) (9, -1.05002097785472869873e-01) (10, -5.94840664416551589966e-03) (11, 1.91371403634548187256e-02) (12, -8.74942019581794738770e-02) (13, 5.66786900162696838379e-02) (14, 6.81358668953180313110e-03) (15, 9.05181542038917541504e-02) (16, -4.52922284603118896484e-02) (17, 1.33474752306938171387e-01) (9, 2.65133418142795562744e-02) (10, -7.50510320067405700684e-02) (11, -8.84806439280509948730e-02) (12, -9.48625802993774414062e-02) (13, -4.03278134763240814209e-02) (14, -7.76882618665695190430e-02) (15, -7.88476616144180297852e-02) (16, -7.35456123948097229004e-02) (17, 1.81945174932479858398e-01) (9, -9.44844260811805725098e-02) (10, 6.67082145810127258301e-02) (11, 4.01133485138416290283e-02) (12, 5.40878921747207641602e-02) (13, -9.44979265332221984863e-02) (14, -4.97146770358085632324e-02) (15, 5.77650256454944610596e-02) (16, -3.90367619693279266357e-02) (17, 2.20503985881805419922e-01) (9, 1.07660919427871704102e-01) (10, 9.41024571657180786133e-02) (11, -1.34652070701122283936e-02) (12, -5.47343045473098754883e-02) (13, -8.36231410503387451172e-02) (14, 6.41586184501647949219e-02) (15, 5.27724623680114746094e-02) (16, -6.36541321873664855957e-02) (17, -1.54523625969886779785e-01) (9, -6.36963471770286560059e-02) (10, -7.97249823808670043945e-02) (11, -9.39950942993164062500e-02) (12, -2.66086123883724212646e-02) (13, 4.57632206380367279053e-02) (14, -5.93333095312118530273e-02) (15, -2.22000339999794960022e-03) (16, 6.42378106713294982910e-02) (17, 1.92180648446083068848e-01) (9, -6.39475649222731590271e-03) (10, 9.42995920777320861816e-02) (11, 3.23054827749729156494e-02) (12, 8.42065364122390747070e-02) (13, -1.01446822285652160645e-01) (14, 8.67606047540903091431e-03) (15, 3.70261594653129577637e-02) (16, 6.56242743134498596191e-02) (17, 2.03609347343444824219e-01) (9, 8.70693325996398925781e-02) (10, 2.59218662977218627930e-02) (11, 8.18083211779594421387e-02) (12, 1.72606501728296279907e-02) (13, 3.48694846034049987793e-02) (14, 6.57290369272232055664e-02) (15, 9.03890281915664672852e-02) (16, 6.64033666253089904785e-02) (17, -1.17759801447391510010e-01) (9, 8.20750221610069274902e-02) (10, 9.82628855854272842407e-03) (11, -8.69762301445007324219e-02) (12, 2.37528141587972640991e-02) (13, 7.31903389096260070801e-02) (14, -8.31815674901008605957e-02) (15, 1.79696902632713317871e-02) (16, -4.62878048419952392578e-02) (17, -1.15112051367759704590e-01) 
