FANN_FLO_2.1
num_layers=3
learning_rate=0.699993
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=15
cascade_activation_steepnesses=1.00000000000000000000e+00 1.15400000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 2.00000000000000000000e+00 6.08100000000000000000e+03 6.08200000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 4.00000000000000000000e+00 1.07597200000000000000e+06 1.07622800000000000000e+06 1.07654500000000000000e+06 1.07680100000000000000e+06 0.00000000000000000000e+00 
layer_sizes=8 8 8 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -1.08366213738918304443e-01) (1, -2.67956294119358062744e-02) (2, 3.97299556061625480652e-03) (3, 1.59528257790952920914e-03) (4, -5.91986551880836486816e-02) (5, 6.62317276000976562500e-02) (6, -7.79423117637634277344e-02) (7, -8.55184420943260192871e-02) (0, -4.43146340548992156982e-02) (1, 6.20628241449594497681e-04) (2, 6.37832283973693847656e-02) (3, -5.17080910503864288330e-02) (4, -1.05691608041524887085e-02) (5, 4.44135554134845733643e-02) (6, 8.98811817169189453125e-02) (7, 3.42604406177997589111e-02) (0, 1.21285244822502136230e-02) (1, 7.27799013257026672363e-02) (2, -5.26295863091945648193e-02) (3, -4.53230217099189758301e-02) (4, -1.05523712933063507080e-01) (5, 8.37828069925308227539e-02) (6, -7.69747868180274963379e-02) (7, -1.95998381823301315308e-02) (0, -8.75781849026679992676e-03) (1, -5.23558855056762695312e-02) (2, -5.71509525179862976074e-02) (3, -2.79222968965768814087e-02) (4, 8.78333970904350280762e-02) (5, 8.78848209977149963379e-02) (6, -4.48843883350491523743e-03) (7, -8.22203084826469421387e-02) (0, 1.43497129902243614197e-02) (1, 1.13232210278511047363e-01) (2, 1.81746240705251693726e-02) (3, 7.85155519843101501465e-02) (4, 7.94639438390731811523e-02) (5, 7.85448178648948669434e-02) (6, -5.37423603236675262451e-02) (7, -6.08694367110729217529e-02) (0, -4.21865172684192657471e-02) (1, 6.97596371173858642578e-02) (2, -2.75255385786294937134e-02) (3, 7.65588060021400451660e-02) (4, 1.41731929033994674683e-02) (5, -3.36630940437316894531e-02) (6, -1.05327190831303596497e-02) (7, -5.94708845019340515137e-02) (0, -3.96253839135169982910e-02) (1, 6.92013427615165710449e-02) (2, 2.05393079668283462524e-02) (3, -3.79521064460277557373e-02) (4, 5.29841482639312744141e-02) (5, 5.77919259667396545410e-02) (6, 6.37058764696121215820e-02) (7, -5.69740310311317443848e-02) (8, -1.11811101436614990234e-01) (9, 1.08204372227191925049e-01) (10, 3.61892255023121833801e-03) (11, 8.44871401786804199219e-02) (12, 8.91209766268730163574e-02) (13, -8.81032273173332214355e-02) (14, -8.73157680034637451172e-02) (15, 1.80289432406425476074e-01) (8, -1.14311851561069488525e-01) (9, 3.37257981300354003906e-02) (10, -3.88317778706550598145e-02) (11, 5.92270642518997192383e-02) (12, 5.76542457565665245056e-03) (13, 5.51646389067173004150e-03) (14, 9.42554026842117309570e-02) (15, 2.67936497926712036133e-01) (8, -2.34243161976337432861e-02) (9, -2.99617536365985870361e-02) (10, 5.96666671335697174072e-02) (11, -7.47908130288124084473e-02) (12, 2.97948922961950302124e-02) (13, -1.23437717556953430176e-02) (14, 2.04245839267969131470e-03) (15, 2.64229238033294677734e-01) (8, -7.43832215666770935059e-02) (9, -7.41260796785354614258e-02) (10, -6.06563352048397064209e-02) (11, 8.07423293590545654297e-02) (12, 7.70533308386802673340e-02) (13, -9.07271876931190490723e-02) (14, -7.22374245524406433105e-02) (15, 2.63266652822494506836e-01) (8, 1.15965582430362701416e-01) (9, 2.21069119870662689209e-02) (10, 7.50289484858512878418e-02) (11, 9.36204269528388977051e-02) (12, 3.15239429473876953125e-02) (13, 6.83813542127609252930e-02) (14, -3.12936082482337951660e-02) (15, -1.61345317959785461426e-01) (8, -1.20130963623523712158e-02) (9, 4.77650761604309082031e-02) (10, -5.23042166605591773987e-03) (11, -9.84390750527381896973e-02) (12, -6.23804181814193725586e-02) (13, -2.27042380720376968384e-03) (14, 1.00181944668292999268e-01) (15, 2.15982869267463684082e-01) (8, 6.59911930561065673828e-02) (9, 5.04786856472492218018e-02) (10, 6.32701665163040161133e-02) (11, -1.52731863781809806824e-02) (12, -6.40398561954498291016e-02) (13, -5.32774403691291809082e-02) (14, -4.51010018587112426758e-02) (15, -2.10551619529724121094e-01) 
