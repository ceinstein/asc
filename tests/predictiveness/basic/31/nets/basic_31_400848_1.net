FANN_FLO_2.1
num_layers=3
learning_rate=0.699993
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=0
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=15
cascade_activation_steepnesses=1.00000000000000000000e+00 1.15400000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 2.00000000000000000000e+00 6.08100000000000000000e+03 6.08200000000000000000e+03 0.00000000000000000000e+00 0.00000000000000000000e+00 4.00000000000000000000e+00 1.07597200000000000000e+06 1.07622800000000000000e+06 1.07654500000000000000e+06 1.07680100000000000000e+06 0.00000000000000000000e+00 
layer_sizes=8 8 8 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (8, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (8, 3, 5.00000000000000000000e-01) (0, 3, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 5.83486482501029968262e-02) (1, -1.91451925784349441528e-02) (2, 8.89715105295181274414e-02) (3, -9.90399159491062164307e-03) (4, 8.34200680255889892578e-02) (5, -9.86058730632066726685e-03) (6, -4.91756945848464965820e-03) (7, 6.91074058413505554199e-02) (0, -4.42106053233146667480e-02) (1, -2.09342455491423606873e-04) (2, -2.99573550000786781311e-03) (3, -1.66550166904926300049e-02) (4, -5.43013140559196472168e-02) (5, 1.17264753207564353943e-02) (6, 4.98749539256095886230e-02) (7, 1.35263931006193161011e-02) (0, 7.48867914080619812012e-02) (1, -7.95200541615486145020e-02) (2, 1.61271095275878906250e-02) (3, -2.24358197301626205444e-02) (4, 4.67607006430625915527e-02) (5, 6.13685622811317443848e-02) (6, 6.53089806437492370605e-02) (7, 6.24570734798908233643e-02) (0, 1.46469101309776306152e-03) (1, -1.12098790705204010010e-02) (2, -4.27436046302318572998e-02) (3, 2.96395178884267807007e-02) (4, -3.61314564943313598633e-02) (5, -3.32185924053192138672e-02) (6, -3.60706113278865814209e-02) (7, 1.04772076010704040527e-01) (0, 4.44791689515113830566e-02) (1, -5.66302426159381866455e-02) (2, -2.89511214941740036011e-02) (3, -3.83394374512135982513e-03) (4, 3.35091724991798400879e-02) (5, 4.86861951649188995361e-02) (6, -3.78835834562778472900e-02) (7, 9.18865203857421875000e-02) (0, -3.88137660920619964600e-02) (1, 5.49540407955646514893e-02) (2, -3.90565693378448486328e-02) (3, -6.48558139801025390625e-04) (4, -3.33194881677627563477e-02) (5, -8.65936726331710815430e-02) (6, -7.44127929210662841797e-02) (7, -5.24771809577941894531e-02) (0, -3.50498817861080169678e-02) (1, 5.67012168467044830322e-02) (2, 1.49655705317854881287e-02) (3, 9.25580188632011413574e-02) (4, 1.80697776377201080322e-02) (5, -1.37699311599135398865e-02) (6, 8.60789269208908081055e-02) (7, 9.57152619957923889160e-02) (8, 6.12550117075443267822e-02) (9, -8.40341970324516296387e-02) (10, 2.03544385731220245361e-02) (11, 8.15306007862091064453e-02) (12, -1.45954480394721031189e-02) (13, 2.07484681159257888794e-02) (14, 8.18929448723793029785e-02) (15, 1.17279209196567535400e-01) (8, 7.93375521898269653320e-02) (9, -5.89742884039878845215e-02) (10, 3.11639159917831420898e-02) (11, -2.09106206893920898438e-02) (12, 9.23283249139785766602e-02) (13, 3.98876592516899108887e-02) (14, -2.42015849798917770386e-02) (15, 1.41971126198768615723e-01) (8, 1.31142940372228622437e-02) (9, 2.51574385911226272583e-02) (10, 6.05299994349479675293e-02) (11, 5.18000088632106781006e-02) (12, 4.09657172858715057373e-02) (13, 4.00965921580791473389e-02) (14, 1.06745481491088867188e-01) (15, 2.63108074665069580078e-01) (8, -1.32962875068187713623e-03) (9, 1.08705982565879821777e-02) (10, 9.15217399597167968750e-02) (11, 7.34252855181694030762e-02) (12, 9.95473936200141906738e-02) (13, 1.59239582717418670654e-02) (14, 6.19033463299274444580e-02) (15, 2.36447125673294067383e-01) (8, 4.22416701912879943848e-02) (9, -4.45311442017555236816e-02) (10, 2.17072665691375732422e-02) (11, -5.31544499099254608154e-02) (12, 9.31921377778053283691e-02) (13, 3.32685559988021850586e-02) (14, -5.71893975138664245605e-02) (15, -1.37234777212142944336e-01) (8, 6.70830458402633666992e-02) (9, 6.15308918058872222900e-02) (10, -2.83300075680017471313e-02) (11, 3.93660292029380798340e-02) (12, 2.33662258833646774292e-02) (13, -1.55036710202693939209e-03) (14, 1.01662293076515197754e-01) (15, 1.27868160605430603027e-01) (8, -6.48667365312576293945e-02) (9, 3.66360694169998168945e-02) (10, 6.75678402185440063477e-02) (11, 9.54249426722526550293e-02) (12, -8.18890705704689025879e-03) (13, 1.05090796947479248047e-01) (14, 5.65529912710189819336e-02) (15, -1.19544170796871185303e-01) 
